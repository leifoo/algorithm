{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Specialization\n",
    "\n",
    "https://www.coursera.org/specializations/algorithms\n",
    "\n",
    "1. Divide and Conquer, Sorting and Searching, and Randomized Algorithms \n",
    "2. Graph Search, Shortest Paths, and Data Structures (https://www.coursera.org/learn/algorithms-graphs-data-structures)\n",
    "3. Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming (https://www.coursera.org/learn/algorithms-greedy)\n",
    "4. Shortest Paths Revisited, NP-Complete Problems and What To Do About Them (https://www.coursera.org/learn/algorithms-npcomplete)\n",
    "\n",
    "## 1. Divide and Conquer, Sorting and Searching, and Randomized Algorithms \n",
    "(https://www.coursera.org/learn/algorithms-divide-conquer)\n",
    "\n",
    "### 1.1 Introduction\n",
    "\n",
    "#### 1.1.1 Why\tStudy Algorithms?\t\n",
    "- important\tfor all other branches of computer science\n",
    "- plays a key role in modern technological innovation\n",
    " - \"Everyone knows Moore's Law - prediction made in 1965 by Intel co-founder Gordon Moore that the density of transistors in integrated circuits would continue to double every 1 to 2 years... in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance grains due to increased processor speed.\" - Excerpt from _Report to the President and Congress: Designing a Digital Future_, December 2010 (page 71). \n",
    "- provides novel \"lens\" on processes outside of computer science and technology\n",
    "- challenging (i.e., good for the brain!)\n",
    "- fun\n",
    "\n",
    "#### 1.1.2 Integer Multiplication\n",
    "\n",
    "Input: 2 n-digit numbers x and y\n",
    "\n",
    "Output: product x*y\n",
    "\n",
    "\"Primitive Operation\" -  add or multiply 2 single-digit numbers\n",
    "\n",
    "The Grad-School Algorithm: roughly n operations per row up to a constant, # of operations overall ~ constant$* n^2$\n",
    "\n",
    "The Algorithm Designer's Mantra\n",
    "<br>\n",
    "\"Perhaps the most important principle for the good algorithm designer is to refuse to be content.\" - Aho, Hopcroft, and Ullman, _The Design and Analysis of Computer Algorithms_, 1974\n",
    "\n",
    "#### 1.1.3 Karatsuba Multiplication\n",
    "\n",
    "A Recursive Algorithm\n",
    "\n",
    "Write $x=10^{n/2}a+b$ and $y=10^{n/2}c+d$ \n",
    "\n",
    "Where $a,b,c,d$ are $n/2$-digit numbers.\n",
    "\n",
    "Then $xy=(10^{n/2}a+b)(10^{n/2}c+d) = 10^{n}ac+10^{n/2}(ad+bc)+bd$ $(*)$\n",
    "\n",
    "Idea: recursively compute ac, ad, bc, bd, then compute $(*)$ in the obvious way.\n",
    "\n",
    "$xy = 10^{n}ac+10^{n/2}(ad+bc)+bd$\n",
    "1. Recursively compute $ac$\n",
    "2. Recursively compute $bd$\n",
    "3. Recursively compute $(a+b)(c+d) = ac+bd+ad+bc$\n",
    "\n",
    "Gauss's Trick $(3)-(1)-(2) = ad+bc$\n",
    "\n",
    "Upshot: Only need 3 recursive multiplications (and some additions)\n",
    "\n",
    "#### 1.1.4 About The Course\n",
    "\n",
    "Course Topics\n",
    "- Vocabulary for design and analysis of algorithms\n",
    "- Divide and conquer algorithm design paradigm\n",
    "- Randomization in algorithm design\n",
    "- Primitives for reasoning about graphs\n",
    "- Use and implementation of data structure\n",
    "\n",
    "Course Topics\n",
    "- Vocabulary for design and analysis of algorithms\n",
    " - E.g., \"Big-Oh\" notation\n",
    " - \"sweet spot\" for high-level reasoning about algorithms\n",
    "- Divide and conquer algorithm design paradigm\n",
    " - Will apply to: Integer multiplication, sorting, matrix multiplication, closet pair\n",
    " - General analysis methods (\"Master Method/Theorem\")\n",
    "- Randomization in algorithm design\n",
    " - Will apply to: QuickSort, primality testing, graph partitioning, hashing\n",
    "- Primitives for reasoning about graphs\n",
    " - connectivity information, shortest path, structure of information and social network\n",
    "- Use and implementation of data structure\n",
    " - Heaps, balanced binary search trees, hashing and some variants (e.g., bloom filters)\n",
    " \n",
    "Topics in Sequel Course\n",
    "- Greedy algorithm design paradigm\n",
    "- Dynamic programming algorithm design paradigm\n",
    "- NP-complete problems and what to do about them\n",
    "- Fast heuristics with provable guarantees\n",
    "- Fast exact algorithms, for special cases\n",
    "- Exact algorithms that beat brute-force search\n",
    "\n",
    "Skills You'll Learn\n",
    "- Become a better programmer\n",
    "- Sharpen your mathematical and analytical skills\n",
    "- Start \"thinking algorithmically\"\n",
    "- Literacy with computer science's \"greatest hits\"\n",
    "- Ace your technical interviews\n",
    "\n",
    "References:\n",
    "- \"Mathematics for Computer Science\", by Eric Lehman and Tom Leighton.\n",
    "- Kleinberg/Tardos,_Algorithm Design_, 2005.\n",
    "- Dasgupta/Papadimitriou/Vazirani,*Algorithms*, 2006.\n",
    "- Cormen/Leiserson/Rivest/Stein,*Introduction to Algorithms*, 2009 (3rd edition).\n",
    "- Mehlhorn/Sanders,*Data Structures and Algorithms: The Basic Toolbox*, 2008.\n",
    "\n",
    "#### 1.1.5 Guiding Principles\n",
    "\n",
    "##### Guiding Principle #1\n",
    "\n",
    "\"Worst - case analysis\": our running time bound holds for every input of length $n$.\n",
    "- Particularly appropriate for \"general-purpose\" routines\n",
    "\n",
    "As Oppposed to \n",
    "- \"average-case\" analysis\n",
    "- benchmarks\n",
    "<br>\n",
    "Both requires domain knowledge\n",
    "\n",
    "BONUS: worst case usually easier to analyze.\n",
    "\n",
    "##### Guiding Principle #2\n",
    "\n",
    "Won't pay much attention to constant factors, lower-order terms.\n",
    "\n",
    "Justifications\n",
    "1. Way easier\n",
    "2. Constants depend on architecture / compiler / programmer anyways\n",
    "3. Lose very little predictive power\n",
    "\n",
    "##### Guiding Principle #3\n",
    "\n",
    "Asymptotic Analysis: focus on running time for large input sizes n \n",
    "\n",
    "Justification: only big problems are interesting!\n",
    "\n",
    "What is a a \"Fast\" algorithm?\n",
    "\n",
    "Adopt these three biases as guiding principles\n",
    "\n",
    "> fast algorithm $\\approx$ worst-case running time grows slowly with input size\n",
    "\n",
    "Usually: want as close to linear $O(n)$ as possible.\n",
    "\n",
    "#### 1.1.6 Merge Sort\n",
    "\n",
    "Why study merge sort?\n",
    "- Good introduction to divide & conquer\n",
    " - Improves over Selection, Insertion, bubble sorts\n",
    "- Motivates guiding principles for algorithm analysis (worst-case and asymptotic analysis)\n",
    "- Analysis generalizes to \"Master Method\"\n",
    "\n",
    "The sorting problem\n",
    "\n",
    "Input: array of n numbers , unsorted\n",
    "<br>\n",
    "Output: same numbers, sorted in increasing order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given array is\n",
      "[12, 11, 13, 5, 6, 7]\n",
      "Sorted array is\n",
      "[5, 6, 7, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "# Python program for implementation of MergeSort \n",
    "  \n",
    "# Merges two subarrays of arr[]. \n",
    "# First subarray is arr[l..m] \n",
    "# Second subarray is arr[m+1..r] \n",
    "def merge(arr, l, m, r): \n",
    "    n1 = m - l + 1\n",
    "    n2 = r - m \n",
    "  \n",
    "    # create temp arrays \n",
    "    L = []\n",
    "    R = [] \n",
    "  \n",
    "    # Copy data to temp arrays L[] and R[] \n",
    "    for i in range(0 , n1): \n",
    "        L.append(arr[l + i]) \n",
    "  \n",
    "    for j in range(0 , n2): \n",
    "        R.append(arr[m + 1 + j]) \n",
    "  \n",
    "    # Merge the temp arrays back into arr[l..r] \n",
    "    i = 0     # Initial index of first subarray \n",
    "    j = 0     # Initial index of second subarray \n",
    "    k = l     # Initial index of merged subarray \n",
    "  \n",
    "    while i < n1 and j < n2 : \n",
    "        if L[i] <= R[j]: \n",
    "            arr[k] = L[i] \n",
    "            i += 1\n",
    "        else: \n",
    "            arr[k] = R[j] \n",
    "            j += 1\n",
    "        k += 1\n",
    "  \n",
    "    # Copy the remaining elements of L[], if there \n",
    "    # are any \n",
    "    while i < n1: \n",
    "        arr[k] = L[i] \n",
    "        i += 1\n",
    "        k += 1\n",
    "  \n",
    "    # Copy the remaining elements of R[], if there \n",
    "    # are any \n",
    "    while j < n2: \n",
    "        arr[k] = R[j] \n",
    "        j += 1\n",
    "        k += 1\n",
    "  \n",
    "# l is for left index and r is right index of the \n",
    "# sub-array of arr to be sorted \n",
    "def mergeSort(arr,l,r): \n",
    "    if l < r: \n",
    "  \n",
    "        # Same as (l+r)/2, but avoids overflow for \n",
    "        # large l and h \n",
    "        m = (l+(r-1))//2\n",
    "  \n",
    "        # Sort first and second halves \n",
    "        mergeSort(arr, l, m) \n",
    "        mergeSort(arr, m+1, r) \n",
    "        merge(arr, l, m, r) \n",
    "  \n",
    "  \n",
    "# Driver code to test above \n",
    "arr = [12, 11, 13, 5, 6, 7] \n",
    "n = len(arr) \n",
    "print (\"Given array is\") \n",
    "print (arr)\n",
    "  \n",
    "mergeSort(arr,0,n-1) \n",
    "print (\"Sorted array is\") \n",
    "print (arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Sort Running Time?\n",
    "\n",
    "running time $\\approx$ # of lines of code executed\n",
    "\n",
    "<font color=red>Upshot:</font> running time of Merge on array of m nujmbers is $\\le 4m + 2 \\le 6m$ \n",
    "\n",
    "<font color=red>Claim:</font> Merge Sort requires $\\le 6n log_2 n + 6n$ operations to sort n numbers. \n",
    "\n",
    "<font color=red>Claim:</font> For every input array of n numbers, Merge Sort produces a sorted output array and uses at most $6n log_2 n + 6n$ operations.\n",
    "\n",
    "Recursion tree have $log_2 n + 1$ levels. At each level $j=0,1,2..., log_2n$, there are $2^j$ subproblems, each of size $n/2^j$.\n",
    "\n",
    "Proof of claim (assuming n = power of 2):\n",
    "\n",
    "At each level $j=0,1,2..., log_2n$,\n",
    "\n",
    "> Total # of operations $\\le 2^j * 6(\\frac{n}{2^j}) = 6n$ at each level\n",
    "\n",
    "> \\# of levels $log_2 n + 1$\n",
    "\n",
    "### 1.2 Asymptotic Analysis\n",
    "\n",
    "#### 1.2.1 The Gist\n",
    "\n",
    "<font color=red>Importance:</font> Vocabulary for the design and analysis of algorithms <font color=blue>(e.g. \"big-Oh\" notation)</font>\n",
    "- \"Sweet spot\" for high-level resoning about algorithms.\n",
    "- Coarse enough to suppress architecture/language/compiler-dependent details.\n",
    "- Sharp enough to make useful comparisons between different algorithms, especially on large inputs (e.g. sorting or integer multiplications).\n",
    "\n",
    "<font color=red>High-level idea:</font> Suppress <font color=blue>constant factors</font> (too system-dependent) and <font color=blue>lower-order terms</font> (irrelevant for large inputs).\n",
    "\n",
    "<font color=red>Example:</font> Equate $6n log_2 n + 6n$ with just $n log n$.\n",
    "\n",
    "<font color=red>Terminology:</font> Running time is $O(nlogn)$, where $n$ = input size\n",
    "\n",
    "#### 1.2.2 Big-Oh: Definition\n",
    "\n",
    "##### English Definition\n",
    "\n",
    "Let $T(n) = $ function on $n=1,2,3,...$ [usually, the worst case running time of an algorithm]\n",
    "\n",
    "Q: When is $T(n) = O(f(n))$?\n",
    "<br>\n",
    "A: if eventually (for all sufficiently large n), $T(n)$ is bounded above by a constant multiple of $f(n)$\n",
    "\n",
    "##### Formal Definition\n",
    "\n",
    "$T(n) = O(f(n))$ if and only if there exist constants $c, n_0 > 0$ such that \n",
    "> $T(n) \\le c\\cdot f(n)$\n",
    "For all $n\\ge n_0$\n",
    "\n",
    "Warning: $c, n_0$ cannot depend on $n$.\n",
    "\n",
    "#### 1.2.3 Big-Oh: Basic Examples\n",
    "\n",
    "Claim: if $T(n) = a_k n^k + ... + a_1 n + a_0$ then \n",
    "> $T(n) = O(n^k)$\n",
    "\n",
    "Claim: for every $k \\ge 1$, $n^k$ is not $O(n^{k-1})$\n",
    "\n",
    "#### 1.2.4 Big-Oh: Relatives (Omega & Theta)\n",
    "\n",
    "##### Omega Notation\n",
    "\n",
    "Definition: $T(n) = \\Omega(f(n))$ \n",
    "\n",
    "If and only if there exist constants  $c, n_0$ such that\n",
    "> $T(n) \\ge c \\cdot f(n)$, $\\forall n \\ge n_0$ \n",
    "\n",
    "##### Theta Notation\n",
    "\n",
    "Definition: $T(n) = \\Theta(f(n))$ \n",
    "\n",
    "If and only if $T(n) = O(f(n))$ and $T(n) = \\Omega(f(n))$\n",
    "\n",
    "Equivalent: there exist constants $c_1, c_2, n_0$ such that\n",
    "> $c_1 f(n) \\le T(n) \\le c_2 f(n)$, $\\forall n \\ge n_0$\n",
    "\n",
    "Let $T(n) = \\frac{1}{2}n^2+3n$, then\n",
    "- $T(n) = \\Omega(n)$\n",
    "- $T(n) = \\Theta(n^2)$\n",
    "- $T(n) = O(n^3)$\n",
    "\n",
    "##### Little-Oh Notation\n",
    "\n",
    "Definition: $T(n) = o(f(n))$ if and only if for all constants $c>0$, there exists a constant $n_0$ such that\n",
    "> $T(n) \\le c \\cdot f(n)$, $\\forall n \\ge n_0$\n",
    "\n",
    "#### 1.2.5 Additional Examples\n",
    "\n",
    "$2^{n+10} = O(2^n)$\n",
    "\n",
    "$2^{10n} \\ne O(2^n)$\n",
    "\n",
    "Claim: for every pair of (positive) function $f(n)$, $g(n)$, $max\\{f,g\\} = \\theta(f(n) + g(n))$\n",
    "\n",
    "### 1.3 Divide and Conquer\n",
    "\n",
    "#### 1.3.1 Closest Pair I\n",
    "\n",
    "Input: a set $P=\\{p1, ..., p_n\\}$ of $n$ points in the plane $R^2$.\n",
    "\n",
    "Notation: $d(p_i, p_j)$ = Euclidean distance\n",
    "\n",
    "So if $p_i = (x_i, y_i)$ and $p_j=(x_j, y_j)$\n",
    "$$ d(p_i, p_j) = \\sqrt{(x_i-x_j)^2 + (y_i-y_j)^2}$$\n",
    "\n",
    "Output: a pair $p^*, q^* \\in P$ of distinct points that minimize $d(p,q)$ over $p$, $q$ in the set $P$\n",
    "\n",
    "Initial Observations\n",
    "\n",
    "Assumption: (for convenience) all points have distinct x-coordinates, distinct y-coordinates\n",
    "\n",
    "Brute-force search: takes $\\Theta(n^2)$ time.\n",
    "\n",
    "1-D Version of Closest Pair:\n",
    "1. Sort points ($O(nlogn)$ time)\n",
    "2. Return closest pair of adjacent points ($O(n)$ time)\n",
    "\n",
    "Goal: $O(nlogn)$ time algorithm for 2-D version.\n",
    "\n",
    "High-Level Approach\n",
    "\n",
    "1. Make copies of points sorted by x-coordinate ($P_x$) and y-coordinate ($P_y$) [$O(nlogn)$ time]\n",
    "2. Use Divide+Conquer\n",
    "\n",
    "The Divide and Conquer Paradigm\n",
    "1. __DIVIDE__ into smaller subproblems.\n",
    "2. __CONQUER__ subproblems recursively.\n",
    "3. __COMBINE__ solutions of subproblems into one for the original problem.\n",
    "\n",
    "$ClosestPair(P_x, P_y)$\n",
    "1. Let $Q$ = left half of $P$, $R$ = right half of $P$. Form $Q_x, Q_y, R_x, R_y$ [takes $O(n)$ time]\n",
    "2. $(p_1, q_1) = ClosestPair(Q_x, Q_y)$\n",
    "3. $(p_2, q_2) = ClosestPair(R_x, R_y)$\n",
    "4. Let $\\delta = min\\{d(p_1, q_1), d(p_2, q_2)\\}$\n",
    "5. $(p_3, q_3) = ClosestSplitPair(P_x, P_y, \\delta)$\n",
    "6. Return best of $(p_1, q_1), (p_2, q_2), (p_3, q_3)$\n",
    "\n",
    "$ClosestSplitPair(P_x, P_y, \\delta)$\n",
    "\n",
    "Let $\\bar x$ = biggest x-coordinate in left of $P$.\n",
    "\n",
    "Let $S_y$ = points of $P$ with x-coordinate in Sorted by y-coordinate\n",
    "\n",
    "Initialize best = $\\delta$, best pair = NULL\n",
    "\n",
    "For i=1 to $|S_y|$-7\n",
    "\n",
    "    For j=1 to 7\n",
    "    \n",
    "        Let $p, q=i^{th}, (i+j)^{th}$ points of $S_y$\n",
    "        \n",
    "        If $d(p, q) <$ best\n",
    "        \n",
    "            best pair = $(p,q)$, best = $d(p,q)$\n",
    "\n",
    "#### 1.3.2 Closest Pair II\n",
    "\n",
    "Correctness Claim\n",
    "\n",
    "Claim: Let $p\\in Q, q \\in R$ be a split pair with $d(p,q) < \\delta$, then\n",
    "- (A) $p$ and $q$ are members of $S_y$\n",
    "- (B) $p$ and $q$ are at most 7 positions apart in $S_y$\n",
    "\n",
    "#### 1.3.3 Counting Inversions I\n",
    "\n",
    "The Problem\n",
    "\n",
    "Input: array $A$ containing the numbers 1, 2, 3, ..., n in some arbitrary order\n",
    "\n",
    "Output: number of inversions = numbers of pairs $(i,j)$ of array indices with $i<j$ and $A[i]>A[j]$\n",
    "\n",
    "Motivation: numerical similarity measure between two ranked list (e.g.: for collaborative filtering)\n",
    "\n",
    "The largest-possible number of inversions that a $n$-element array can have is $C^{2}_{n}$\n",
    "\n",
    "High-Level Approach\n",
    "\n",
    "Brute-force: $\\Theta(n^2)$ time\n",
    "\n",
    "Can we do better? \n",
    "\n",
    "__KEY IDEA #1__: Divide + Conquer\n",
    "\n",
    "Call an inversion (i,j) [with i<j]\n",
    "\n",
    "    Left: if i,j < n/2\n",
    "    Right: if i,j > n/2\n",
    "    Split: if i<=n/2<j\n",
    "\n",
    "Count (array A, length n)\n",
    "    if n=1, return 0\n",
    "    else\n",
    "        X = Count (1st half of A, n/2)\n",
    "        Y = COunt (2nd half of A, n/2)\n",
    "        Z = CountSplitInv(A,n)\n",
    "    return x+y+z\n",
    "\n",
    "Goal: implement CountSplitInv in linear ($O(n)$) time then count will run in $O(nlogn)$ time.\n",
    "\n",
    "#### 1.3.4 Counting Inversion II\n",
    "\n",
    "KEY IDEA # 2: have recursive calls both count inversions and sort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Graph Search, Shortest Paths, and Data Structures \n",
    "(https://www.coursera.org/learn/algorithms-graphs-data-structures)\n",
    "## 3. Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming \n",
    "(https://www.coursera.org/learn/algorithms-greedy)\n",
    "## 4. Shortest Paths Revisited, NP-Complete Problems and What To Do About Them \n",
    "(https://www.coursera.org/learn/algorithms-npcomplete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
